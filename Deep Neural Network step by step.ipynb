{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v4 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(5.0, 4.0) \n",
    "plt.rcParams['image.interpolation']='nearest'\n",
    "plt.rcParams['image.cmap']='gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initilize_parameters(n_x,n_h,n_y):\n",
    "    \"\"\"\n",
    "    number of units in:\n",
    "    n_x: input layer\n",
    "    n_h: hidden layer\n",
    "    n_y: output layer\n",
    "    Returns: \n",
    "    dictionary of parameters: W1, b1, W2, b2\n",
    "    \"\"\"\n",
    "    \n",
    "    W1=np.random.randn(n_h, n_x)*0.01\n",
    "    b1=np.zeros((n_h,1))\n",
    "    W2=np.random.randn(n_y, n_h)*0.01\n",
    "    b2=np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape==(n_h, n_x))\n",
    "    assert(b1.shape==(n_h,1))\n",
    "    assert(W2.shape==(n_y, n_h))\n",
    "    assert(b2.shape==(n_y,1))\n",
    "    \n",
    "    parameters={'W1':W1, 'b1':b1,'W2':W2, 'b2':b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter W1:\n",
      "[[ 0.00698032 -0.00447129  0.01224508  0.00403492  0.00593579]\n",
      " [-0.01094912  0.00169382  0.00740556 -0.00953701 -0.00266219]\n",
      " [ 0.00032615 -0.01373117  0.00315159  0.00846161 -0.00859516]]\n",
      "Parameter b1:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Parameter W2:\n",
      "[[ 0.00350546 -0.01312283 -0.00038696]]\n",
      "Parameter b2\n",
      ":[[0.]]\n"
     ]
    }
   ],
   "source": [
    "# test: \n",
    "parameters=initilize_parameters(5,3,1)\n",
    "print('Parameter W1:\\n{0}\\nParameter b1:\\n{1}\\nParameter W2:\\n{2}\\nParameter b2\\n:{3}'.format(parameters['W1'], parameters['b1'], \n",
    "                                                  parameters['W2'], parameters['b2']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_param_deep(layer_dims):\n",
    "    '''\n",
    "    input: size of each layer\n",
    "    output: dictionary of parameters for each layer\n",
    "    '''\n",
    "    \n",
    "    parameters={}\n",
    "    L=len(layer_dims)\n",
    "    \n",
    "    for i in range(1,L):\n",
    "        parameters['W{}'.format(i)]=np.random.randn(layer_dims[i], layer_dims[i-1])*0.01\n",
    "        parameters['b{}'.format(i)]=np.zeros((layer_dims[i], 1))\n",
    "        \n",
    "        assert(parameters['W{}'.format(i)].shape==(layer_dims[i], layer_dims[i-1]))\n",
    "        assert(parameters['b{}'.format(i)].shape==(layer_dims[i],1))\n",
    "        \n",
    "    return parameters       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter W1:\n",
      "[[-0.00997027  0.00248799 -0.00296641  0.00495211 -0.00174703]\n",
      " [ 0.00986335  0.00213534  0.021907   -0.01896361 -0.00646917]\n",
      " [ 0.00901487  0.02528326 -0.00248635  0.00043669 -0.00226314]\n",
      " [ 0.01331457 -0.00287308  0.0068007  -0.00319802 -0.01272559]]\n",
      "Parameter b1:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Parameter W2:\n",
      "[[ 0.00313548  0.00503185  0.01293226 -0.00110447]]\n",
      "Parameter b2\n",
      ":[[0.]]\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "parameters=init_param_deep([5,4,1])\n",
    "print('Parameter W1:\\n{0}\\nParameter b1:\\n{1}\\nParameter W2:\\n{2}\\nParameter b2\\n:{3}'.format(parameters['W1'], parameters['b1'], \n",
    "                                                  parameters['W2'], parameters['b2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward (A,W,b):\n",
    "    ''' input: \n",
    "       A - activation from previous layer, shape: (size of prev. layer, number of examples)\n",
    "       W - weights matrix, shape: (size of current layer, size of previous layer)\n",
    "       b - bias vector, shape: (size of current layer, 1)\n",
    "       returns:\n",
    "       Z - pre-activation parameter\n",
    "       cache - dictionary, stores A, W, b for backpropagation  \n",
    "   '''\n",
    "    Z=np.dot(W,A)+b\n",
    "    \n",
    "    assert(Z.shape==(W.shape[0], A.shape[1]))\n",
    "    cache=(A,W,b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter Z:\n",
      "[[0.80822865 2.69075905]]\n",
      "cache: (array([[-0.61736206,  0.5627611 ],\n",
      "       [ 0.24073709,  0.28066508],\n",
      "       [-0.0731127 ,  1.16033857]]), array([[0.36949272, 1.90465871, 1.1110567 ]]), array([[0.6590498]]))\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "A = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1) \n",
    "\n",
    "a,b=linear_forward(A,W,b)\n",
    "print('parameter Z:\\n{0}\\ncache: {1}'.format(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid (Z):\n",
    "    '''\n",
    "    inputs: Z - activation function\n",
    "    returns: \n",
    "    A - matrix, shape same as Z\n",
    "    cache - Z (for backpropagation)\n",
    "    '''\n",
    "    A=1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    assert (A.shape==Z.shape)\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu (Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    cache=Z\n",
    "    assert(A.shape==Z.shape)\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W,b,activation):\n",
    "    '''\n",
    "    inputs: \n",
    "    A_prev: activation from previous layer, shape: (size of prev. layer, number of examples)\n",
    "    W - weights, shape: (size of current layer, size of previous layer)\n",
    "    b - bias vector, shape: (size of current layer, 1)\n",
    "    activation: string 'sigmoid' or 'relu'\n",
    "    returns: \n",
    "    A - activation function matrix,\n",
    "    cache - dictionary, stores 'linear_cache' and 'activation_cache' for backpropagation\n",
    "    '''\n",
    "    \n",
    "    Z,linear_cache=linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation=='sigmoid': \n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    elif activation=='relu':\n",
    "        A, activation_cache=relu (Z)\n",
    "        \n",
    "    assert(A.shape==(W.shape[0], A_prev.shape[1]))\n",
    "    cache=(linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid A=[[0.58224635 0.73673706]]\n",
      "With ReLu A=[[0.33200174 1.02907776]]\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "a = np.random.randn(3,2)\n",
    "b = np.random.randn(1,3)\n",
    "c = np.random.randn(1,1) \n",
    "\n",
    "A, linear_activation_cache=linear_activation_forward(a,b,c, activation='sigmoid')\n",
    "print ('With sigmoid A={0}'.format(A))\n",
    "A,linear_activation_cache=linear_activation_forward(a,b,c,activation='relu')\n",
    "print ('With ReLu A={0}'.format(A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    '''\n",
    "    input: \n",
    "    X - data, shape(input size, number of examples)\n",
    "    parameters: from init_param_deep()\n",
    "    return:\n",
    "    AL - last activation function\n",
    "    caches - list of caches (from linear_activation_forward())\n",
    "    '''\n",
    "    \n",
    "    caches=[]\n",
    "    A=X\n",
    "    L=len(parameters)//2\n",
    "    \n",
    "    #relu first\n",
    "    for i in range (1,L):\n",
    "        A_prev=A\n",
    "        A,cache=linear_activation_forward(A_prev, parameters['W{}'.format(i)],\n",
    "                                          parameters['b{}'.format(i)], \n",
    "                                          activation='relu')\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache=linear_activation_forward(A, parameters['W{}'.format(i)],\n",
    "                                          parameters['b{}'.format(i)], \n",
    "                                          activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "    assert(AL.shape==(1,X.shape[1]))\n",
    "        \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-b60e8810e0dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL_model_forward_test_case_2hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AL = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Length of caches list = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-1533b8734200>\u001b[0m in \u001b[0;36mL_model_forward\u001b[1;34m(X, parameters)\u001b[0m\n\u001b[0;32m     23\u001b[0m     AL, cache=linear_activation_forward(A, parameters['W{}'.format(i)],\n\u001b[0;32m     24\u001b[0m                                           \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                                           activation='sigmoid')\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mcaches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-6ae208f04483>\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[1;34m(A_prev, W, b, activation)\u001b[0m\n\u001b[0;32m     11\u001b[0m     '''\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinear_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlinear_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-34827d413d5e>\u001b[0m in \u001b[0;36mlinear_forward\u001b[1;34m(A, W, b)\u001b[0m\n\u001b[0;32m      8\u001b[0m        \u001b[0mcache\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstores\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbackpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m    '''\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mZ\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "# test\n",
    "def L_model_forward_test_case_2hidden():\n",
    "    np.random.seed(6)\n",
    "    X = np.random.randn(5,4)\n",
    "    W1 = np.random.randn(4,5)\n",
    "    b1 = np.random.randn(4,1)\n",
    "    W2 = np.random.randn(3,4)\n",
    "    b2 = np.random.randn(3,1)\n",
    "    W3 = np.random.randn(1,3)\n",
    "    b3 = np.random.randn(1,1)\n",
    "  \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3} \n",
    "    return X, parameters\n",
    "\n",
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
